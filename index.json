[
	{
		"authors": [
			"admin"
		],
		"categories": null,
		"content": "Hello, I am Shuang Chen (陈爽). I earned my Ph.D. in Computer Engineering at Cornell University, under the supervision of Prof. José F. Martínez and extensive collaboration with Prof. Christina Delimitrou. My thesis is about QoS-aware resource management of interactive services in datacenters.\nI joined Shuhai Lab at Huawei Cloud as a researcher in Aug 2021, working on workload characterization and software-hardware co-optimization in the cloud.\n",
		"date": -62135596800,
		"expirydate": -62135596800,
		"kind": "taxonomy",
		"lang": "en",
		"lastmod": -62135596800,
		"objectID": "481ee1424e61a542df874f28cd544844",
		"permalink": "https://sc2682cornell.github.io/authors/shuang/",
		"publishdate": "0001-01-01T00:00:00Z",
		"relpermalink": "/authors/shuang/",
		"section": "authors",
		"summary": "Hello, I am Shuang Chen (陈爽). I earned my Ph.D. in Computer Engineering at Cornell University, under the supervision of Prof. José F. Martínez and extensive collaboration with Prof. Christina Delimitrou. My thesis is about QoS-aware resource management of interactive services in datacenters.\nI joined Shuhai Lab at Huawei Cloud as a researcher in Aug 2021, working on workload characterization and software-hardware co-optimization in the cloud.",
		"tags": null,
		"title": "Shuang Chen",
		"type": "authors"
	},
	{
		"authors": [
			"*Yajuan Peng",
			"***Shuang Chen**(*equal contribution)",
			"Yi Zhao",
			"Zhibin Yu"
		],
		"categories": null,
		"content": "",
		"date": 1706745600,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1706745600,
		"objectID": "ccea0a174c1a9cf84e2ad3962165694b",
		"permalink": "https://sc2682cornell.github.io/publication/ufo/",
		"publishdate": "2024-02-01T00:00:00Z",
		"relpermalink": "/publication/ufo/",
		"section": "publication",
		"summary": "Public clouds typically adopt (1) multi-tenancy to increase server utilization; (2) virtualization to provide isolation between different tenants; (3) oversubscription of resources to further increase resource efficiency. However, prior work all focuses on optimizing one or two elements, and fails to considerately bring QoS-aware multi-tenancy, virtualization and resource oversubscription together. We find three challenges when the three elements coexist. First, the double scheduling symptoms are $10\times$ worse with latency-critical (LC) workloads which are comprised of numerous sub-millisecond tasks and are significantly different from conventional batch applications. Second, inner-VM resource contention also exists between threads of the same VM when running LC applications, calling for inner-VM core isolation. Third, no application-level performance metrics can be obtained by the host to guide resource management in realistic public clouds. To address these challenges, we propose a QoS-aware core manager dubbed UFO to specifically support co-location of multiple LC workloads in virtualized and oversubscribed public cloud environments. UFO solves the three above-mentioned challenges, by (1) coordinating the guest and host CPU cores (vCPU-pCPU coordination), and (2) doing fine-grained inner-VM resource isolation, to push core management in realistic public clouds to the extreme. Compared with the state-of-the-art core managers, it saves up to 50% of physical cores under the same co-location scenario, and is able to cover up to 5.7x more co-location scenarios under the same amount of core resources.",
		"tags": null,
		"title": "UFO: The Ultimate QoS-Aware CPU Core Management for Virtualized and Oversubscribed Public Cloud",
		"type": "publication"
	},
	{
		"authors": [
			"**Shuang Chen**",
			"Angela Jin",
			"Christina Delimitrou",
			"José F. Martínez"
		],
		"categories": null,
		"content": "",
		"date": 1648771200,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1648771200,
		"objectID": "36d453e22cf55abf555d5b8345851684",
		"permalink": "https://sc2682cornell.github.io/publication/retail/",
		"publishdate": "2022-04-01T00:00:00Z",
		"relpermalink": "/publication/retail/",
		"section": "publication",
		"summary": "Many cloud services have QoS requirements, with most requests needing to complete within a given latency constraint. Recently, researchers have begun to investigate whether it is possible to meet QoS for these latency-critical applications while attempting to save power on a per-request basis. Existing work shows that one can indeed hand-tune a request latency predictor offline for a particular cloud application, and consult it at runtime to modulate CPU voltage and frequency, resulting in substantial power savings. In this paper, we propose ReTail, an automated and general solution for request-level power management of latency-critical services with QoS constraints. We present a systematic process to select the features of any given application that best correlate with its request latency. ReTail uses these features to predict latency, and adjust a CPU's power consumption. ReTail's predictor is trained fully at runtime. We show that unlike previous findings, simple techniques perform better than complex machine learning models, when using the right input application features. For a web search engine, ReTail outperforms prior mechanisms based on complex hand-tuned predictors for that application domain. Furthermore, ReTail's systematic approach also yields superior power savings across a diverse set of cloud applications.",
		"tags": null,
		"title": "ReTail: Opting for Learning Simplicity to Enable QoS-Aware Power Management in the Cloud",
		"type": "publication"
	},
	{
		"authors": [
			"**Shuang Chen**",
			"Yi Jiang",
			"Christina Delimitrou",
			"José F. Martínez"
		],
		"categories": null,
		"content": "",
		"date": 1648771200,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1648771200,
		"objectID": "21d766e2944ee713c4ef385beadd4b66",
		"permalink": "https://sc2682cornell.github.io/publication/pimcloud/",
		"publishdate": "2022-04-01T00:00:00Z",
		"relpermalink": "/publication/pimcloud/",
		"section": "publication",
		"summary": "The slowdown of Moore's Law, combined with advances in 3D stacking of logic and memory, have pushed architects to revisit the concept of processing-in-memory (PIM) to overcome the memory wall bottleneck. This PIM renaissance finds itself in a very different computing landscape from the one twenty years ago, as more and more computation shifts to the cloud. Most PIM architecture papers still focus on best-effort applications, while PIM's impact on latency-critical cloud applications is not well understood. This paper explores how datacenters can exploit PIM architectures in the context of latency-critical applications. We adopt a general-purpose cloud server with HBM-based, 3D-stacked logic+memory modules, and study the impact of PIM on six diverse interactive cloud applications. We reveal the previously neglected opportunity that PIM presents to these services, and show the importance of properly managing PIM-related resources to meet the QoS targets of interactive services and maximize resource efficiency. Then, we present PIMCloud, the first QoS-aware resource manager designed for cloud systems with PIM allowing colocation of multiple latency-critical and best-effort applications. We show that PIMCloud efficiently manages PIM resources: it (1) improves effective machine utilization by up to 70% and 85% (average 24% and 33%) under 2-app and 3-app mixes, compared to the best state-of-the-art manager; (2) helps latency-critical applications meet QoS; and (3) adapts to varying load patterns.  ",
		"tags": null,
		"title": "PIMCloud: QoS-Aware Resource Management of Latency-Critical Applications in Clouds with Processing-in-Memory",
		"type": "publication"
	},
	{
		"authors": [
			"**Shuang Chen**",
			"Christina Delimitrou",
			"José F. Martínez"
		],
		"categories": null,
		"content": "",
		"date": 1555113600,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1555113600,
		"objectID": "6bdf7a8f8e90832f72f5be944821c8fa",
		"permalink": "https://sc2682cornell.github.io/publication/parties/",
		"publishdate": "2019-04-13T00:00:00Z",
		"relpermalink": "/publication/parties/",
		"section": "publication",
		"summary": "Multi-tenancy in modern datacenters is currently limited to a single latency-critical, interactive service, running alongside one or more low-priority, best-effort jobs. This limits the efficiency gains from multi-tenancy, especially as most cloud applications progressively shift from batch jobs to services with strict latency requirements. We present PARTIES, a QoS-aware resource manager that enables an arbitrary number of interactive, latency-critical services to share a physical node without QoS violations. PARTIES leverages a set of hardware and software resource partitioning mechanisms to adjust allocations dynamically at runtime, to meet QoS of each co-scheduled workload and maximize throughput for the machine. We evaluate PARTIES on state-of-the-art server platforms across a set of diverse interactive services. Our results show that PARTIES improves throughput under QoS by 61% on average compared to existing resource managers, and that the rate of improvement increases with the number of co-scheduled applications per physical host. ",
		"tags": null,
		"title": "PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services",
		"type": "publication"
	},
	{
		"authors": [
			"**Shuang Chen**",
			"Shay GalOn",
			"Christina Delimitrou",
			"Srilatha Manne",
			"José F. Martínez"
		],
		"categories": null,
		"content": "",
		"date": 1506816000,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1506816000,
		"objectID": "2605986e1ba59e2dc08752fb3b9fca8f",
		"permalink": "https://sc2682cornell.github.io/publication/iiswc17/",
		"publishdate": "2017-10-01T00:00:00Z",
		"relpermalink": "/publication/iiswc17/",
		"section": "publication",
		"summary": "Key-value stores (e.g., Memcached) and web servers (e.g., NGINX) are widely used by cloud providers. As interactive services, they have strict service-level objectives, with typical 99th-percentile tail latencies on the order of a few milliseconds. Unlike average latency, tail latency is more sensitive to changes in usage load and traffic patterns, system configurations, and resource availability. Understanding the sensitivity of tail latency to application and system factors is critical to efficiently design and manage systems for these latency-critical services. We present a comprehensive study of the impact a diverse set of application, hardware, and isolation configurations have on tail latency for two representative interactive services, Memcached and NGINX. Examined factors include input load, thread-level parallelism, request size, virtualization, and resource partitioning. We conduct this study on two server platforms with significant differences in terms of architecture and price points: an Intel Xeon and an ARM-based Cavium ThunderX server. Experimental results show that latency on both platforms is subject to changes of several orders of magnitude depending on application and system settings, with Cavium ThunderX being more sensitive to configuration parameters. ",
		"tags": null,
		"title": "Workload Characterization of Interactive Cloud Services on Big and Small Server Platforms",
		"type": "publication"
	},
	{
		"authors": [
			"Xiaodong Wang",
			"**Shuang Chen**",
			"Jeff Setter",
			"José F. Martínez"
		],
		"categories": null,
		"content": "",
		"date": 1486166400,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1486166400,
		"objectID": "da92418f8320f927834095768d6d9ebd",
		"permalink": "https://sc2682cornell.github.io/publication/swap/",
		"publishdate": "2017-02-04T00:00:00Z",
		"relpermalink": "/publication/swap/",
		"section": "publication",
		"summary": "Performance isolation is an important goal in server-class environments. Partitioning the last-level cache of a chip multiprocessor (CMP) across co-running applications has proven useful in this regard. Two popular approaches are (a) hardware support for way partitioning, or (b) operating system support for set partitioning through page coloring. Unfortunately, neither approach by itself is scalable beyond a handful of cores without incurring in significant performance overheads. We propose SWAP, a scalable and fine-grained cache management technique that seamlessly combines set and way partitioning. By cooperatively managing cache ways and sets, SWAP (“Set and WAy Partitioning”) can successfully provide hundreds of fine-grained cache partitions for the manycore era. SWAP requires no additional hardware beyond way partitioning. In fact, SWAP can be readily implemented in existing commercial servers whose processors do support hardware way partitioning. In this paper, we prototype SWAP on a 48-core Cavium ThunderX platform running Linux, and show average speedups over no cache partitioning are twice as large as those attained with hardware way partitioning alone.",
		"tags": null,
		"title": "SWAP: Effective Fine-grain Management of Shared Last-level Caches with Minimum Hardware Support",
		"type": "publication"
	},
	{
		"authors": [
			"**Shuang Chen**",
			"Shunning Jiang",
			"Bingsheng He",
			"Xueyan Tang"
		],
		"categories": null,
		"content": "",
		"date": 1467244800,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1467244800,
		"objectID": "926c9c2a445e1b657117d507c32d7a8a",
		"permalink": "https://sc2682cornell.github.io/publication/sort/",
		"publishdate": "2016-06-30T00:00:00Z",
		"relpermalink": "/publication/sort/",
		"section": "publication",
		"summary": "Hardware evolution has been one of the driving factors for the redesign of database systems. Recently, approximate storage emerges in the area of computer architecture. It trades off precision for better performance and/or energy consumption. Previous studies have demonstrated the benefits of approximate storage for applications that are tolerant to imprecision such as image processing. However, it is still an open question whether and how approximate storage can be used for applications that do not expose such intrinsic tolerance. In this paper, we study one of the most basic operations in database--sorting on a hybrid storage system with both precise storage and approximate storage. Particularly, we start with a study of three common sorting algorithms on approximate storage. Experimental results show that a 95% sorted sequence can be obtained with up to 40% reduction in total write latencies. Thus, we propose an approx-refine execution mechanism to improve the performance of sorting algorithms on the hybrid storage system to produce precise results. Our optimization gains the performance benefits by offloading the sorting operation to approximate storage, followed by an efficient refinement to resolve the unsortedness on the output of the approximate storage. Our experiments show that our approx-refine can reduce the total memory access time by up to 11%. These studies shed light on the potential of approximate hardware for improving the performance of applications that require precise results.",
		"tags": null,
		"title": "A Study of Sorting Algorithms on Approximate Memory",
		"type": "publication"
	},
	{
		"authors": [
			"Naifeng Jing",
			"**Shuang Chen**",
			"Shunning Jiang",
			"Li Jiang",
			"Chao Li",
			"Xiaoyao Liang"
		],
		"categories": null,
		"content": "",
		"date": 1437523200,
		"expirydate": -62135596800,
		"kind": "page",
		"lang": "en",
		"lastmod": 1437523200,
		"objectID": "47a47552678617c867480c5ac95d0e94",
		"permalink": "https://sc2682cornell.github.io/publication/bank/",
		"publishdate": "2015-07-22T00:00:00Z",
		"relpermalink": "/publication/bank/",
		"section": "publication",
		"summary": "Modern General Purpose Graphic Processing Unit (GPGPU) demands a large Register File (RF), which is typically organized into multiple banks to support the massive parallelism. Although heavy banking benefits RF throughput, its associated area and energy costs with diminishing performance gains greatly limit future RF s-caling. In this paper, we propose an improved RF design with a bank stealing technique, which enables a high RF throughput with compact area. By deeply investigating the GPGPU microarchitecture, we identify the deficiency in the state-of-the-art RF designs as the bank conflict problem, while the majority of conflicts can be eliminated leveraging the fact that the highly-banked RF oftentimes experiences under-utilization. This is especially true in GPGPU where multiple ready warps are available at the scheduling stage with their operands to be wisely coordinated. Our lightweight bank stealing technique can opportunistically fill the idle banks for better operand service, and the average GPGPU performance can be improved under smaller energy budget with significant area saving, which makes it promising for sustainable RF scaling.",
		"tags": null,
		"title": "Bank stealing for conflict mitigation in GPGPU Register File",
		"type": "publication"
	}
]